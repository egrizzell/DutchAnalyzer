{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5965d7d4",
   "metadata": {},
   "source": [
    "# Extracting words and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f938c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import ujson\n",
    "from dutchanalyzer.config import *\n",
    "from dutchanalyzer.utils import *\n",
    "from dutchanalyzer.json_utils import *\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "import re\n",
    "from pprint import pprint\n",
    "import ast\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ac77353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fe28fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_save_path = Path(WIKT_PREPROCESSING_DIR, 'en')\n",
    "nld_save_path = Path(WIKT_PREPROCESSING_DIR, 'nl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3155001",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_save_path = Path(WIKT_PREPROCESSING_DIR, '12-11-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2de21b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today().__format__(\"%d-%m-%y\")\n",
    "current_save_folder = Path(INTERIM_DATA_DIR, 'cleaning', 'wikt', str(today))\n",
    "folders = {'en': ['EEF', 'ENF'], 'nl':['NEF', 'NNF']}\n",
    "\n",
    "\n",
    "for k, v in folders.items():\n",
    "    for f in v:\n",
    "        Path.mkdir(Path(current_save_folder, k, f), parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65261309",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = Path(current_save_folder, 'all_words.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bfd187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "NNR_file = Path(NNR_DIR, 'NNR.jsonl')\n",
    "NER_file = Path(NER_DIR, 'NER.jsonl')\n",
    "EER_file = Path(EER_DIR, 'EER.jsonl')\n",
    "ENR_file = Path(ENR_DIR, 'ENR.jsonl')\n",
    "\n",
    "NNF_folder = Path(WIKT_CLEANING_DIR, 'nl', 'NNF')\n",
    "NEF_folder = Path(WIKT_CLEANING_DIR, 'nl', 'NEF')\n",
    "EEF_folder = Path(WIKT_CLEANING_DIR, 'en', 'EEF')\n",
    "ENF_folder = Path(WIKT_CLEANING_DIR, 'en', 'ENF')\n",
    "\n",
    "all_words_file = Path(WIKT_CLEANING_DIR, 'all_words.jsonl')\n",
    "eef_words_file = Path(EEF_folder, 'eef_words.jsonl')\n",
    "enf_words_file = Path(ENF_folder, 'enf_words.jsonl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d08e8f",
   "metadata": {},
   "source": [
    "## Extracting Words/Pos/Senses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e2d0f",
   "metadata": {},
   "source": [
    "- Extract words and parts of speech to dict, add all senses to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4955d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words_senses(raw_entry: dict):\n",
    "    word = raw_entry.get(\"word\")\n",
    "    pos = raw_entry.get(\"pos\", 'unknown')\n",
    "    senses = raw_entry.get(\"senses\")\n",
    "    translations = raw_entry.get(\"translations\")\n",
    "    lang_code = raw_entry.get(\"lang_code\")\n",
    "    forms = raw_entry.get('forms')\n",
    "    if not forms:\n",
    "        forms = raw_entry.get('form_of')\n",
    "    glosses = []\n",
    "    sense_translations = []\n",
    "    word_entry = {'word': word,\n",
    "                  'pos': pos, \n",
    "                  'lang_code': lang_code}\n",
    "    #word_entry = {word: {pos: {'lang_code': lang_code}}}\n",
    "\n",
    "    if senses:\n",
    "        word_entry['senses'] = {}\n",
    "        for i, sense in enumerate(senses):\n",
    "            new_sense = {}\n",
    "            if 'glosses' in sense:\n",
    "                glosses = sense['glosses']\n",
    "                new_sense['glosses'] = glosses\n",
    "            if 'translations' in sense or 'translation' in sense:\n",
    "                sense_translations = sense['translations']\n",
    "                if not sense_translations:\n",
    "                    sense_translations = sense_translations['translation']\n",
    "                new_sense['translations'] = sense_translations\n",
    "            if 'form_of' in sense or 'forms' in sense:\n",
    "                forms = sense.get('form_of')\n",
    "                if not forms: forms = sense['forms']\n",
    "                new_sense['forms'] = forms\n",
    "                \n",
    "            word_entry['senses'][i] = new_sense\n",
    "\n",
    "    if translations:\n",
    "        word_entry['translations'] = translations\n",
    "    if forms:\n",
    "        word_entry['forms'] = forms\n",
    "    if 'wl_code' in raw_entry:\n",
    "        word_entry['wl_code'] = raw_entry['wl_code']\n",
    "    if 'etymology_templates' in raw_entry:\n",
    "        word_entry['etymology_templates'] = raw_entry['etymology_templates']\n",
    "    return word_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a94800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_obj(obj):\n",
    "    lang_codes_to_keep = ['nl', 'en', 'simple', 'ang', 'dum', 'nds', 'odt', 'nds-nl', 'enm']\n",
    "    new_senses = []\n",
    "    senses = obj.get('senses')\n",
    "    if senses:\n",
    "        for sense in senses:\n",
    "            if 'attestations' in sense:\n",
    "                sense.pop('attestations')\n",
    "            # if 'examples' in sense:\n",
    "            #     sense.pop('examples')\n",
    "            if 'categories' in sense:\n",
    "                cats = [c for c in sense['categories'] if not ('Pages with' in c and ('entries' in c or 'entry' in c))]\n",
    "                sense['categories'] = cats\n",
    "            new_senses.append(sense)\n",
    "\n",
    "        obj['senses'] = new_senses\n",
    "    if 'etymology_templates' in obj:\n",
    "        new_templates = []\n",
    "        for template in obj['etymology_templates']:\n",
    "            if 'args' in template:\n",
    "                if template['args'].get('1', '') in lang_codes_to_keep:\n",
    "                    new_templates.append(template)\n",
    "        if new_templates:\n",
    "            obj['etymology_templates'] = new_templates\n",
    "    if 'translations' in obj:\n",
    "        translations = obj['translations']\n",
    "        new_translations = [t for t in translations if t.get(\"lang_code\") in lang_codes_to_keep]\n",
    "        obj['translations'] = translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa97fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_dict = {}\n",
    "repeat_words = []\n",
    "entries_batch = []\n",
    "error_lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "977ca896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140758it [00:27, 5117.59it/s] \n"
     ]
    }
   ],
   "source": [
    "batch = []\n",
    "entries_batch = []\n",
    "file = ENR_file\n",
    "wl_code = 'ENF'\n",
    "out_file = Path(WIKT_CLEANING_DIR, 'en', 'ENF.jsonl')\n",
    "ENF_definitions_file = Path(WIKT_CLEANING_DIR, 'en', 'ENF_definitions.jsonl')\n",
    "with open(file, 'r', encoding='utf-8') as f:\n",
    "    with open(out_file, 'w+', encoding='utf-8') as out:\n",
    "        for i, line in tqdm(enumerate(f)):\n",
    "            loaded = json.loads(line)\n",
    "           \n",
    "            if loaded:\n",
    "                try:\n",
    "                    filter_obj(loaded)\n",
    "                    loaded['wl_code'] = wl_code\n",
    "                    entries_batch.append(loaded)\n",
    "                    word = loaded.get('word')\n",
    "                    word_entry = extract_words_senses(loaded)\n",
    "                    batch.append(word_entry)\n",
    "                    if len(entries_batch) > 1000:\n",
    "                            for obj in entries_batch:\n",
    "                                json.dump(obj, out, ensure_ascii=False)\n",
    "                                out.write('\\n')\n",
    "                            entries_batch = []\n",
    "     \n",
    "                except Exception as e:\n",
    "                    error_lines.append((i, loaded))\n",
    "                    display(line)\n",
    "                    print(\"Error on line: \", i, \" Error: \", e)\n",
    "                \n",
    "        if entries_batch:\n",
    "            for obj in entries_batch:\n",
    "                json.dump(obj, out, ensure_ascii=False)\n",
    "                out.write('\\n')  \n",
    "with open(ENF_definitions_file, 'w+', encoding='utf-8') as out:\n",
    "    for obj in batch:\n",
    "        json.dump(obj, out, ensure_ascii=False)\n",
    "        out.write('\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd64675e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'woordenboek',\n",
       " 'pos': 'noun',\n",
       " 'lang_code': 'nl',\n",
       " 'senses': {'glosses': ['dictionary']},\n",
       " 'wl_code': 'ENF'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'word': 'gratis',\n",
       " 'pos': 'adj',\n",
       " 'lang_code': 'nl',\n",
       " 'senses': {'glosses': ['free, without charge']},\n",
       " 'wl_code': 'ENF'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'word': 'gratuit',\n",
       " 'pos': 'adj',\n",
       " 'lang_code': 'nl',\n",
       " 'senses': {'glosses': ['gratuitous, not obliged to']},\n",
       " 'wl_code': 'ENF'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'word': 'word',\n",
       " 'pos': 'verb',\n",
       " 'lang_code': 'nl',\n",
       " 'senses': {'glosses': ['inflection of worden:', 'imperative']},\n",
       " 'wl_code': 'ENF'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'word': 'pond',\n",
       " 'pos': 'noun',\n",
       " 'lang_code': 'nl',\n",
       " 'senses': {'glosses': ['one of several monetary units', 'Flemish pound']},\n",
       " 'wl_code': 'ENF'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'word': 'pies',\n",
       " 'pos': 'noun',\n",
       " 'lang_code': 'nl',\n",
       " 'senses': {'glosses': ['alternative form of pis; pee, piss']},\n",
       " 'wl_code': 'ENF'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'word': 'A',\n",
       " 'pos': 'character',\n",
       " 'lang_code': 'nl',\n",
       " 'senses': {'glosses': ['The first letter of the Dutch alphabet, written in the Latin script.']},\n",
       " 'wl_code': 'ENF'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'word': 'raven',\n",
       " 'pos': 'verb',\n",
       " 'lang_code': 'nl',\n",
       " 'senses': {'glosses': ['to (hold a) rave, to party wildly']},\n",
       " 'wl_code': 'ENF'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'word': 'raven',\n",
       " 'pos': 'noun',\n",
       " 'lang_code': 'nl',\n",
       " 'senses': {'glosses': ['obsolete form of raaf']},\n",
       " 'wl_code': 'ENF'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'word': 'raven',\n",
       " 'pos': 'noun',\n",
       " 'lang_code': 'nl',\n",
       " 'senses': {'glosses': ['plural of raaf']},\n",
       " 'wl_code': 'ENF'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-senses:  0\n",
      "No gloss entries:  0\n",
      "Multi-glosses:  [{'word': 'word', 'pos': 'verb', 'lang_code': 'nl', 'senses': {'glosses': ['inflection of worden:', 'imperative']}, 'wl_code': 'ENF'}, {'word': 'pond', 'pos': 'noun', 'lang_code': 'nl', 'senses': {'glosses': ['one of several monetary units', 'Flemish pound']}, 'wl_code': 'ENF'}]\n",
      "One Gloss:  8\n"
     ]
    }
   ],
   "source": [
    "multi_senses = []\n",
    "enf_one_gloss_senses = []\n",
    "enf_multigloss_senses = []\n",
    "no_gloss_entries = []\n",
    "\n",
    "for item in batch[0:10]:\n",
    "    display(item)\n",
    "    if 'senses' in item:\n",
    "        if len(item['senses']) > 1:\n",
    "            multi_senses.append(item)\n",
    "        else:\n",
    "            sense = item['senses']\n",
    "            if sense:\n",
    "                glosses = sense.get('glosses')\n",
    "                if not glosses:\n",
    "                    no_gloss_entries.append(item)\n",
    "                elif len(glosses) > 1:\n",
    "                    enf_multigloss_senses.append(item)\n",
    "                else:\n",
    "                    enf_one_gloss_senses.append(item)\n",
    "print(\"Multi-senses: \", len(multi_senses))  \n",
    "print(\"No gloss entries: \", len(no_gloss_entries))  \n",
    "print(\"Multi-glosses: \", enf_multigloss_senses)    \n",
    "print(\"One Gloss: \", len(enf_one_gloss_senses))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1d565de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb535884",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnf_entries_batch = []\n",
    "nef_entries_batch = []\n",
    "enf_entries_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ceabe318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1417383it [03:01, 7829.03it/s] \n",
      "1417383it [00:43, 32380.19it/s]\n"
     ]
    }
   ],
   "source": [
    "batch = []\n",
    "error_lines = []\n",
    "entries_batch = []\n",
    "file = EER_file\n",
    "wl_code = 'EEF'\n",
    "entries_file = Path(current_save_folder, 'EEF.jsonl')\n",
    "eef_definitions_file = Path(WIKT_CLEANING_DIR, 'en', 'EEF_definitions.jsonl')\n",
    "with open(file, 'r', encoding='utf-8') as f:\n",
    "    with open(entries_file, 'w+', encoding='utf-8') as out:\n",
    "        for i, line in tqdm(enumerate(f)):\n",
    "            loaded = json.loads(line)\n",
    "            if loaded:\n",
    "                try:\n",
    "                    filter_obj(loaded)\n",
    "                    loaded['wl_code'] = wl_code\n",
    "                    entries_batch.append(loaded)\n",
    "                    word = loaded.get('word')\n",
    "                    word_entry = extract_words_senses(loaded)\n",
    "                    \n",
    "                    batch.append(word_entry)\n",
    "                    if len(entries_batch) > 1000:\n",
    "                        for obj in entries_batch:\n",
    "                            json.dump(obj, out, ensure_ascii=False)\n",
    "                            out.write('\\n')\n",
    "                        entries_batch = []\n",
    "                except Exception as e:\n",
    "                    error_lines.append((i, loaded))\n",
    "                    display(line)\n",
    "                    print(\"Error on line: \", i, \" Error: \", e)\n",
    "                \n",
    "        if entries_batch:\n",
    "            for obj in entries_batch:\n",
    "                json.dump(obj, out, ensure_ascii=False)\n",
    "                out.write('\\n')   \n",
    "if batch:\n",
    "    with open(eef_definitions_file, 'w+', encoding='utf-8') as eout:\n",
    "        for i, obj in tqdm(enumerate(batch)):\n",
    "            json.dump(obj, eout, ensure_ascii=False)\n",
    "            eout.write('\\n')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9d1f8a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting Lines: 100%|██████████| 1.63G/1.63G [00:01<00:00, 1.30GB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1423864"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_lines_with_progress(EER_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9168207d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17441it [00:00, 17789.55it/s]\n"
     ]
    }
   ],
   "source": [
    "batch = []\n",
    "entries_batch = []\n",
    "file = NER_file\n",
    "wl_code = 'NEF'\n",
    "out_file = Path(WIKT_CLEANING_DIR, 'nl', 'NEF.jsonl')\n",
    "definitions_file = Path(WIKT_CLEANING_DIR, 'nl', 'NEF_definitions.jsonl')\n",
    "with open(file, 'r', encoding='utf-8') as f:\n",
    "    with open(out_file, 'w+', encoding='utf-8') as out:\n",
    "        for i, line in tqdm(enumerate(f)):\n",
    "            loaded = json.loads(line)\n",
    "           \n",
    "            if loaded:\n",
    "                try:\n",
    "                    filter_obj(loaded)\n",
    "                    loaded['wl_code'] = wl_code\n",
    "                    entries_batch.append(loaded)\n",
    "                    word = loaded.get('word')\n",
    "                    word_entry = extract_words_senses(loaded)\n",
    "                    batch.append(word_entry)\n",
    "                    if len(entries_batch) > 1000:\n",
    "                            for obj in entries_batch:\n",
    "                                json.dump(obj, out, ensure_ascii=False)\n",
    "                                out.write('\\n')\n",
    "                            entries_batch = []\n",
    "     \n",
    "                except Exception as e:\n",
    "                    error_lines.append((i, loaded))\n",
    "                    display(line)\n",
    "                    print(\"Error on line: \", i, \" Error: \", e)\n",
    "                \n",
    "        if entries_batch:\n",
    "            for obj in entries_batch:\n",
    "                json.dump(obj, out, ensure_ascii=False)\n",
    "                out.write('\\n')  \n",
    "with open(definitions_file, 'w+', encoding='utf-8') as out:\n",
    "    for obj in batch:\n",
    "        json.dump(obj, out, ensure_ascii=False)\n",
    "        out.write('\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f59dc85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "611444it [01:20, 7606.56it/s] \n"
     ]
    }
   ],
   "source": [
    "batch = []\n",
    "entries_batch = []\n",
    "file = NNR_file\n",
    "wl_code = 'NNF'\n",
    "out_file = Path(WIKT_CLEANING_DIR, 'nl', 'NNF.jsonl')\n",
    "definitions_file = Path(WIKT_CLEANING_DIR, 'nl', 'NNF_definitions.jsonl')\n",
    "with open(file, 'r', encoding='utf-8') as f:\n",
    "    with open(out_file, 'w+', encoding='utf-8') as out:\n",
    "        for i, line in tqdm(enumerate(f)):\n",
    "            loaded = json.loads(line)\n",
    "           \n",
    "            if loaded:\n",
    "                try:\n",
    "                    filter_obj(loaded)\n",
    "                    loaded['wl_code'] = wl_code\n",
    "                    entries_batch.append(loaded)\n",
    "                    word = loaded.get('word')\n",
    "                    word_entry = extract_words_senses(loaded)\n",
    "                    batch.append(word_entry)\n",
    "                    if len(entries_batch) > 1000:\n",
    "                            for obj in entries_batch:\n",
    "                                json.dump(obj, out, ensure_ascii=False)\n",
    "                                out.write('\\n')\n",
    "                            entries_batch = []\n",
    "     \n",
    "                except Exception as e:\n",
    "                    error_lines.append((i, loaded))\n",
    "                    display(line)\n",
    "                    print(\"Error on line: \", i, \" Error: \", e)\n",
    "                \n",
    "        if entries_batch:\n",
    "            for obj in entries_batch:\n",
    "                json.dump(obj, out, ensure_ascii=False)\n",
    "                out.write('\\n')  \n",
    "with open(definitions_file, 'w+', encoding='utf-8') as out:\n",
    "    for obj in batch:\n",
    "        json.dump(obj, out, ensure_ascii=False)\n",
    "        out.write('\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f62685",
   "metadata": {},
   "source": [
    "### Parse Repeats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
