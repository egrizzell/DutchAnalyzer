{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5195acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ujson\n",
    "from dutchanalyzer.config import *\n",
    "from dutchanalyzer.utils import *\n",
    "from dutchanalyzer.json_utils import *\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from io import StringIO\n",
    "import datetime\n",
    "import re\n",
    "from pprint import pprint\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7668a38",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "73b0830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ERAW_FILE = Path(RAW_KAIKKI_DIR, 'en', 'kaikki_en-raw-wiktextract-data.jsonl') \n",
    "NRAW_FILE = Path(RAW_KAIKKI_DIR, 'nl', 'kaikki_nl-raw-extract.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5ccde3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_save_path = Path(WIKT_PREPROCESSING_DIR, 'en')\n",
    "nld_save_path = Path(WIKT_PREPROCESSING_DIR, 'nl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2818dd",
   "metadata": {},
   "source": [
    "## Process Raw NL File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bb0cde",
   "metadata": {},
   "source": [
    "### Intake JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cc46900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "NNR_lines_file = Path(NNR_DIR, 'NNR_lines.json')\n",
    "NER_lines_file = Path(NER_DIR, 'NER_lines.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "894dbb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The nl file is small enough to intake by readlines\n",
    "nl_lines = []\n",
    "file_path = NRAW_FILE\n",
    "with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    nl_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c12752",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_word_lines = []\n",
    "nl_non_word_lines = []\n",
    "for line in nl_lines:\n",
    "    if line[0:7] == '{\"word\"':\n",
    "        nl_word_lines.append(line)\n",
    "    else:\n",
    "        nl_non_word_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "988904ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "NNR_lines = []\n",
    "NER_lines = []\n",
    "other_lines = []\n",
    "\n",
    "for line in nl_word_lines:\n",
    "    if '\"lang\": \"Engels\"' in line[0:100]:\n",
    "        NER_lines.append(line)\n",
    "    elif '\"lang_code\": \"en\"' in line[0:100]:\n",
    "        NER_lines.append(line)\n",
    "    elif '\"lang_code\": \"nl\"' in line[0:100]:\n",
    "        NNR_lines.append(line)\n",
    "    elif '\"lang\": \"Nederlands\"' in line[0:100]:\n",
    "        NNR_lines.append(line)\n",
    "    elif '\"lang_code\":' not in line[0:100]:\n",
    "        other_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8cc290",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NNR_lines_file, 'w', encoding='utf-8') as f:\n",
    "    for line in NNR_lines:\n",
    "        f.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa83f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NER_lines_file, 'w', encoding='utf-8') as f:\n",
    "    for line in NER_lines:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fe5db3",
   "metadata": {},
   "source": [
    "## Process Raw EN File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf677855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting Lines: 100%|██████████| 21.3G/21.3G [00:14<00:00, 1.45GB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10329308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_en_lines = count_lines_with_progress(ERAW_FILE)\n",
    "print(total_en_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9692579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_save_path = Path(WIKT_PREPROCESSING_DIR, '07-11-25')\n",
    "chunks_dir = Path(current_save_path, 'en', 'chunks2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e35aec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def make_structure_line_tuple(key, line):\n",
    "    size = 0\n",
    "    if line:\n",
    "        size = len(line)\n",
    "        obj_type = type(line)\n",
    "        if size == 0:\n",
    "            return (key, obj_type, 0, 0)\n",
    "        counts = Counter()\n",
    "        typecounts = Counter(type(x).__name__ for x in line)\n",
    "        if isinstance(line, dict):\n",
    "            items = line.items()\n",
    "            return (key, dict, size, typecounts) \n",
    "        elif isinstance(line, list):\n",
    "            return (key, list, size, typecounts)\n",
    "        elif isinstance(line, str):\n",
    "            try:\n",
    "                line = json.loads(line)\n",
    "                if isinstance(line, str):\n",
    "                    return (key, str, size, typecounts)\n",
    "            except:\n",
    "                \n",
    "                return (key, str, size, typecounts)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6ffe5ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_cjk_or_arabic_fast(text: str, limit: int = 50) -> bool:\n",
    "    \"\"\"Return True if the first `limit` characters contain\n",
    "    any Chinese, Japanese, Korean, or Arabic/Farsi character.\"\"\"\n",
    "    for ch in text[:limit]:\n",
    "        cp = ord(ch)\n",
    "        # CJK (Chinese/Japanese/Korean)\n",
    "        if (\n",
    "            0x4E00 <= cp <= 0x9FFF or  # CJK Unified Ideographs\n",
    "            0x3400 <= cp <= 0x4DBF or  # CJK Ext A\n",
    "            0xF900 <= cp <= 0xFAFF or  # CJK Compatibility\n",
    "            0x3040 <= cp <= 0x30FF or  # Hiragana + Katakana\n",
    "            0x31F0 <= cp <= 0x31FF or  # Katakana Extensions\n",
    "            0xAC00 <= cp <= 0xD7AF or  # Hangul Syllables\n",
    "            # Arabic / Farsi\n",
    "            0x0600 <= cp <= 0x06FF or\n",
    "            0x0750 <= cp <= 0x077F or\n",
    "            0x08A0 <= cp <= 0x08FF or\n",
    "            0xFB50 <= cp <= 0xFEFF\n",
    "        ):\n",
    "            return True  # stop immediately\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "4b4a3a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import isin\n",
    "\n",
    "\n",
    "def keep_obj(obj: dict) -> bool:\n",
    "    if \"lang_code\" not in obj:\n",
    "        return False\n",
    "    if obj[\"lang_code\"] not in [\"nl\", \"en\"]:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def keep_obj_before_load(obj_str: str) -> bool:\n",
    "    if has_cjk_or_arabic_fast(obj_str):\n",
    "        return False\n",
    "    \n",
    "    if obj_str.find('\"lang_code\": \"en\"') == -1 and obj_str.find('\"lang_code\": \"nl\"') == -1:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    return True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c97d0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subkeys(line):\n",
    "    structure_dict = {}\n",
    "    line_type = type(line)\n",
    "    if not line:\n",
    "        return line_type\n",
    "    elif isinstance(line, str):\n",
    "        return str\n",
    "    elif isinstance(line, int):\n",
    "        return int\n",
    "    elif isinstance(line, dict):\n",
    "        for k, v in line.items():\n",
    "            structure_dict[k] = get_subkeys(v)\n",
    "    elif isinstance(line, list):\n",
    "        subkeys_list = []\n",
    "        substruct_count = []\n",
    "        for i, v in enumerate(line):\n",
    "            substruct = (dict, get_subkeys(v))\n",
    "            if substruct not in subkeys_list:\n",
    "                subkeys_list.append(substruct)\n",
    "                substruct_count.append(1)\n",
    "            else:\n",
    "                index = subkeys_list.index(substruct)\n",
    "                substruct_count[index] += 1\n",
    "        return (f'unique:{len(subkeys_list)}', subkeys_list)\n",
    "    else:\n",
    "        print(line)\n",
    "    return structure_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b96a1314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subkeysV2(line):\n",
    "    structure_dict = {}\n",
    "    line_type = type(line)\n",
    "    if not line:\n",
    "        return line_type\n",
    "    elif isinstance(line, str):\n",
    "        return str\n",
    "    elif isinstance(line, int):\n",
    "        return int\n",
    "    elif isinstance(line, dict):\n",
    "        for k, v in line.items():\n",
    "            structure_dict[k] = get_subkeysV2(v)\n",
    "    elif isinstance(line, list):\n",
    "        subkeys_list = []\n",
    "        substruct_count = []\n",
    "        line_tuple_list = []\n",
    "        keys_set = set()\n",
    "        for i, v in enumerate(line):\n",
    "            substruct = get_subkeysV2(v)\n",
    "            if isinstance(substruct, dict):\n",
    "                keys_set.update(substruct.keys())\n",
    "            if substruct not in subkeys_list:\n",
    "                subkeys_list.append(substruct)\n",
    "                \n",
    "                substruct_count.append(1)\n",
    "            else:\n",
    "                index = subkeys_list.index(substruct)\n",
    "                substruct_count[index] += 1\n",
    "        return (keys_set, subkeys_list)\n",
    "    else:\n",
    "        print(line)\n",
    "    return structure_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e7e70fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurse_subkeys(structure, depth=0):\n",
    "    indent = '  ' * depth\n",
    "    if isinstance(structure, dict):\n",
    "        for key, value in structure.items():\n",
    "            print(f\"{indent}{key}:\")\n",
    "            recurse_subkeys(value, depth + 1)\n",
    "    elif isinstance(structure, list):\n",
    "        for i, item in enumerate(structure):\n",
    "            print(f\"{indent}- Item {i}:\")\n",
    "            recurse_subkeys(item, depth + 1)\n",
    "    else:\n",
    "        print(f\"{indent}{structure}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf4c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_structure_analysis(file_path: Path, out_path: Path, batch_size: int = 100):\n",
    "    batch_list = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        batch_counter = 0\n",
    "        for line in f:\n",
    "            if not keep_obj_before_load(line):\n",
    "                continue\n",
    "            obj = ujson.loads(line)\n",
    "            if not keep_obj(obj):\n",
    "                continue\n",
    "            \n",
    "            subkeys = get_subkeysV2(obj)\n",
    "            \n",
    "            display(subkeys)\n",
    "            #batch_list.append(subkeys)\n",
    "            batch_counter += 1\n",
    "            if batch_counter >= batch_size:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "66cfaf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def safe_dict(obj_str: str):\n",
    "    if isinstance(obj_str, str):\n",
    "        try:\n",
    "            return ast.literal_eval(obj_str)\n",
    "        except Exception:\n",
    "            return \"\"       # fallback\n",
    "        \n",
    "def filter_translations_regex(obj_str: str):\n",
    "    translations_pattern = r'\"translations\"\\s*:\\s*\\[({.*?})\\]'\n",
    "    dutch_translation_pattern = r'\\{[^{}]*?\"lang\"\\s*:\\s*\"Dutch\"[^{}]*?\\}'\n",
    "    translations_block = re.compile(translations_pattern, re.DOTALL)\n",
    "    dutch_object = re.compile(dutch_translation_pattern, re.DOTALL)\n",
    "    match = translations_block.search(obj_str)\n",
    "    \n",
    "    while match is not None:\n",
    "        start, end = match.span()\n",
    "        m = match.group(0)\n",
    "        dn = dutch_object.findall(m)\n",
    "        if dn:\n",
    "            dn = [safe_dict(x) for x in dn]\n",
    "            str_dn = '[' + ', '.join(json.dumps(x) for x in dn) + ']'\n",
    "            obj_str = obj_str[:start] + '\"translations\": ' + str_dn + obj_str[end:]\n",
    "            match = translations_block.search(obj_str, start + len('\"translations\": ' + str(dn)))\n",
    "        else:\n",
    "            \n",
    "            to_remove_end = end\n",
    "            if end < len(obj_str) and obj_str[end] == ',':\n",
    "                to_remove_end += 1\n",
    "            obj_str = obj_str[:start] + obj_str[to_remove_end:]\n",
    "            match = translations_block.search(obj_str, start)\n",
    "\n",
    "    return obj_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_categories_regex(obj_str: str):\n",
    "    categories_pattern = r'\"categories\"\\s*:\\s*\\[.*?\\]'\n",
    "    categories_block = re.compile(categories_pattern, re.DOTALL)\n",
    "    terms_with_Dtranslations = re.compile(r'\"Terms with Dutch translations\"', re.DOTALL)\n",
    "    terms_with_translations = re.compile(r'\"Terms with \\w* translations\"', re.DOTALL)\n",
    "    match = categories_block.search(obj_str)\n",
    "    \n",
    "    while match is not None:\n",
    "        start, end = match.span()\n",
    "        # Remove the entire categories block including any trailing comma\n",
    "        to_remove_end = end\n",
    "        if end < len(obj_str) and obj_str[end] == ', ':\n",
    "            to_remove_end += 1\n",
    "        obj_str = obj_str[:start] + obj_str[to_remove_end:]\n",
    "        match = categories_block.search(obj_str, start)\n",
    "\n",
    "    return obj_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c0afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_line(obj_str: str) -> str:\n",
    "    obj_str = filter_translations_regex(obj_str)\n",
    "    return obj_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "53b2f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_obj(obj: dict):\n",
    "    if \"categories\" in obj:\n",
    "        categories = obj[\"categories\"]\n",
    "        new_categories = []\n",
    "        if isinstance(categories, list):\n",
    "            for i, cat in enumerate(categories):\n",
    "                if isinstance(cat, str):\n",
    "                    if cat not in [\"Terms with Dutch translations\", \"Terms with English translations\"]:\n",
    "                        if cat.startswith('Terms with') and cat.endswith('translations'):\n",
    "                            print(cat)\n",
    "                            categories.remove(cat)\n",
    "                        else:\n",
    "                            new_categories.append(cat)\n",
    "        obj[\"categories\"] = new_categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "03f5c9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 30/10329308 [00:00<3:28:20, 826.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms with Abaza translations\n",
      "Terms with Adyghe translations\n",
      "Terms with Albanian translations\n",
      "Terms with Ambonese Malay translations\n",
      "Terms with Amharic translations\n",
      "Terms with Antillean Creole translations\n",
      "Terms with Aragonese translations\n",
      "Terms with Armenian translations\n",
      "Terms with Assamese translations\n",
      "Terms with Asturian translations\n",
      "Terms with Aymara translations\n",
      "Terms with Balkan Romani translations\n",
      "Terms with Bambara translations\n",
      "Terms with Basque translations\n",
      "Terms with Bengali translations\n",
      "Terms with Bikol Central translations\n",
      "Terms with Breton translations\n",
      "Terms with Burmese translations\n",
      "Terms with Cantonese translations\n",
      "Terms with Carpathian Rusyn translations\n",
      "Terms with Cebuano translations\n",
      "Terms with Chakma translations\n",
      "Terms with Chechen translations\n",
      "Terms with Choctaw translations\n",
      "Terms with Chuvash translations\n",
      "Terms with Classical Syriac translations\n",
      "Terms with Corsican translations\n",
      "Terms with Czech translations\n",
      "Terms with Dargwa translations\n",
      "Terms with Dungan translations\n",
      "Terms with Dzongkha translations\n",
      "Terms with Eastern Mari translations\n",
      "Terms with Esperanto translations\n",
      "Terms with Evenki translations\n",
      "Terms with Faroese translations\n",
      "Terms with Forest Enets translations\n",
      "Terms with French translations\n",
      "Terms with Fula translations\n",
      "Terms with Galician translations\n",
      "Terms with Georgian translations\n",
      "Terms with German translations\n",
      "Terms with Greenlandic translations\n",
      "Terms with Guerrero Amuzgo translations\n",
      "Terms with Haitian Creole translations\n",
      "Terms with Hausa translations\n",
      "Terms with Hebrew translations\n",
      "Terms with Hiligaynon translations\n",
      "Terms with Hokkien translations\n",
      "Terms with Hungarian translations\n",
      "Terms with Icelandic translations\n",
      "Terms with Igbo translations\n",
      "Terms with Inari Sami translations\n",
      "Terms with Ingrian translations\n",
      "Terms with Interlingua translations\n",
      "Terms with Irish translations\n",
      "Terms with Japanese translations\n",
      "Terms with Juba Arabic translations\n",
      "Terms with Kabyle translations\n",
      "Terms with Kalo Finnish Romani translations\n",
      "Terms with Kapampangan translations\n",
      "Terms with Karakalpak translations\n",
      "Terms with Kashubian translations\n",
      "Terms with Kazakh translations\n",
      "Terms with Khiamniungan Naga translations\n",
      "Terms with Kikuyu translations\n",
      "Terms with Komi-Zyrian translations\n",
      "Terms with Korean translations\n",
      "Terms with Kven translations\n",
      "Terms with Ladin translations\n",
      "Terms with Lak translations\n",
      "Terms with Latgalian translations\n",
      "Terms with Latvian translations\n",
      "Terms with Leonese translations\n",
      "Terms with Ligurian translations\n",
      "Terms with Lithuanian translations\n",
      "Terms with Livvi translations\n",
      "Terms with Lower Sorbian translations\n",
      "Terms with Lule Sami translations\n",
      "Terms with Lü translations\n",
      "Terms with Malagasy translations\n",
      "Terms with Malayalam translations\n",
      "Terms with Manchu translations\n",
      "Terms with Manx translations\n",
      "Terms with Mapudungun translations\n",
      "Terms with Marathi translations\n",
      "Terms with Mauritian Creole translations\n",
      "Terms with Middle French translations\n",
      "Terms with Mirandese translations\n",
      "Terms with Mon translations\n",
      "Terms with Montagnais translations\n",
      "Terms with Nanai translations\n",
      "Terms with Naxi translations\n",
      "Terms with Nepali translations\n",
      "Terms with Nogai translations\n",
      "Terms with North Frisian translations\n",
      "Terms with Northern Mansi translations\n",
      "Terms with Norwegian Bokmål translations\n",
      "Terms with Nǀuu translations\n",
      "Terms with Odia translations\n",
      "Terms with Old Armenian translations\n",
      "Terms with Oromo translations\n",
      "Terms with Ottoman Turkish translations\n",
      "Terms with Papiamentu translations\n",
      "Terms with Pennsylvania German translations\n",
      "Terms with Piedmontese translations\n",
      "Terms with Polish translations\n",
      "Terms with Punjabi translations\n",
      "Terms with Romanian translations\n",
      "Terms with Russian translations\n",
      "Terms with Sakizaya translations\n",
      "Terms with Samogitian translations\n",
      "Terms with Sardinian translations\n",
      "Terms with Scots translations\n",
      "Terms with Serbo-Croatian translations\n",
      "Terms with Shona translations\n",
      "Terms with Sicilian translations\n",
      "Terms with Sindhi translations\n",
      "Terms with Sinte Romani translations\n",
      "Terms with Slovak translations\n",
      "Terms with Somali translations\n",
      "Terms with Southern Altai translations\n",
      "Terms with Southern Sami translations\n",
      "Terms with Spanish translations\n",
      "Terms with Swabian translations\n",
      "Terms with Swazi translations\n",
      "Terms with Tabasaran translations\n",
      "Terms with Tagalog translations\n",
      "Terms with Tamil translations\n",
      "Terms with Tatar translations\n",
      "Terms with Tetum translations\n",
      "Terms with Tibetan translations\n",
      "Terms with Tok Pisin translations\n",
      "Terms with Tulu translations\n",
      "Terms with Turkish translations\n",
      "Terms with Tuvan translations\n",
      "Terms with Ukrainian translations\n",
      "Terms with Urdu translations\n",
      "Terms with Uzbek translations\n",
      "Terms with Venetan translations\n",
      "Terms with Vietnamese translations\n",
      "Terms with Volapük translations\n",
      "Terms with Võro translations\n",
      "Terms with Walloon translations\n",
      "Terms with Welsh translations\n",
      "Terms with West Frisian translations\n",
      "Terms with Western Mari translations\n",
      "Terms with Wu translations\n",
      "Terms with Yakut translations\n",
      "Terms with Yoruba translations\n",
      "Terms with Zhuang translations\n",
      "Terms with Abaza translations\n",
      "Terms with Adyghe translations\n",
      "Terms with Albanian translations\n",
      "Terms with Ambonese Malay translations\n",
      "Terms with Amharic translations\n",
      "Terms with Antillean Creole translations\n",
      "Terms with Aragonese translations\n",
      "Terms with Armenian translations\n",
      "Terms with Assamese translations\n",
      "Terms with Asturian translations\n",
      "Terms with Aymara translations\n",
      "Terms with Balkan Romani translations\n",
      "Terms with Bambara translations\n",
      "Terms with Basque translations\n",
      "Terms with Bengali translations\n",
      "Terms with Bikol Central translations\n",
      "Terms with Breton translations\n",
      "Terms with Burmese translations\n",
      "Terms with Cantonese translations\n",
      "Terms with Carpathian Rusyn translations\n",
      "Terms with Cebuano translations\n",
      "Terms with Chakma translations\n",
      "Terms with Chechen translations\n",
      "Terms with Choctaw translations\n",
      "Terms with Chuvash translations\n",
      "Terms with Classical Syriac translations\n",
      "Terms with Corsican translations\n",
      "Terms with Czech translations\n",
      "Terms with Dargwa translations\n",
      "Terms with Dungan translations\n",
      "Terms with Dzongkha translations\n",
      "Terms with Eastern Mari translations\n",
      "Terms with Esperanto translations\n",
      "Terms with Evenki translations\n",
      "Terms with Faroese translations\n",
      "Terms with Forest Enets translations\n",
      "Terms with French translations\n",
      "Terms with Fula translations\n",
      "Terms with Galician translations\n",
      "Terms with Georgian translations\n",
      "Terms with German translations\n",
      "Terms with Greenlandic translations\n",
      "Terms with Guerrero Amuzgo translations\n",
      "Terms with Haitian Creole translations\n",
      "Terms with Hausa translations\n",
      "Terms with Hebrew translations\n",
      "Terms with Hiligaynon translations\n",
      "Terms with Hokkien translations\n",
      "Terms with Hungarian translations\n",
      "Terms with Icelandic translations\n",
      "Terms with Igbo translations\n",
      "Terms with Inari Sami translations\n",
      "Terms with Ingrian translations\n",
      "Terms with Interlingua translations\n",
      "Terms with Irish translations\n",
      "Terms with Japanese translations\n",
      "Terms with Juba Arabic translations\n",
      "Terms with Kabyle translations\n",
      "Terms with Kalo Finnish Romani translations\n",
      "Terms with Kapampangan translations\n",
      "Terms with Karakalpak translations\n",
      "Terms with Kashubian translations\n",
      "Terms with Kazakh translations\n",
      "Terms with Khiamniungan Naga translations\n",
      "Terms with Kikuyu translations\n",
      "Terms with Komi-Zyrian translations\n",
      "Terms with Korean translations\n",
      "Terms with Kven translations\n",
      "Terms with Ladin translations\n",
      "Terms with Lak translations\n",
      "Terms with Latgalian translations\n",
      "Terms with Latvian translations\n",
      "Terms with Leonese translations\n",
      "Terms with Ligurian translations\n",
      "Terms with Lithuanian translations\n",
      "Terms with Livvi translations\n",
      "Terms with Lower Sorbian translations\n",
      "Terms with Lule Sami translations\n",
      "Terms with Lü translations\n",
      "Terms with Malagasy translations\n",
      "Terms with Malayalam translations\n",
      "Terms with Manchu translations\n",
      "Terms with Manx translations\n",
      "Terms with Mapudungun translations\n",
      "Terms with Marathi translations\n",
      "Terms with Mauritian Creole translations\n",
      "Terms with Middle French translations\n",
      "Terms with Mirandese translations\n",
      "Terms with Mon translations\n",
      "Terms with Montagnais translations\n",
      "Terms with Nanai translations\n",
      "Terms with Naxi translations\n",
      "Terms with Nepali translations\n",
      "Terms with Nogai translations\n",
      "Terms with North Frisian translations\n",
      "Terms with Northern Mansi translations\n",
      "Terms with Norwegian Bokmål translations\n",
      "Terms with Nǀuu translations\n",
      "Terms with Odia translations\n",
      "Terms with Old Armenian translations\n",
      "Terms with Oromo translations\n",
      "Terms with Ottoman Turkish translations\n",
      "Terms with Papiamentu translations\n",
      "Terms with Pennsylvania German translations\n",
      "Terms with Piedmontese translations\n",
      "Terms with Polish translations\n",
      "Terms with Punjabi translations\n",
      "Terms with Romanian translations\n",
      "Terms with Russian translations\n",
      "Terms with Sakizaya translations\n",
      "Terms with Samogitian translations\n",
      "Terms with Sardinian translations\n",
      "Terms with Scots translations\n",
      "Terms with Serbo-Croatian translations\n",
      "Terms with Shona translations\n",
      "Terms with Sicilian translations\n",
      "Terms with Sindhi translations\n",
      "Terms with Sinte Romani translations\n",
      "Terms with Slovak translations\n",
      "Terms with Somali translations\n",
      "Terms with Southern Altai translations\n",
      "Terms with Southern Sami translations\n",
      "Terms with Spanish translations\n",
      "Terms with Swabian translations\n",
      "Terms with Swazi translations\n",
      "Terms with Tabasaran translations\n",
      "Terms with Tagalog translations\n",
      "Terms with Tamil translations\n",
      "Terms with Tatar translations\n",
      "Terms with Tetum translations\n",
      "Terms with Tibetan translations\n",
      "Terms with Tok Pisin translations\n",
      "Terms with Tulu translations\n",
      "Terms with Turkish translations\n",
      "Terms with Tuvan translations\n",
      "Terms with Ukrainian translations\n",
      "Terms with Urdu translations\n",
      "Terms with Uzbek translations\n",
      "Terms with Venetan translations\n",
      "Terms with Vietnamese translations\n",
      "Terms with Volapük translations\n",
      "Terms with Võro translations\n",
      "Terms with Walloon translations\n",
      "Terms with Welsh translations\n",
      "Terms with West Frisian translations\n",
      "Terms with Western Mari translations\n",
      "Terms with Wu translations\n",
      "Terms with Yakut translations\n",
      "Terms with Yoruba translations\n",
      "Terms with Zhuang translations\n",
      "Terms with Abkhaz translations\n",
      "Terms with Albanian translations\n",
      "Terms with Ancient Greek translations\n",
      "Terms with Armenian translations\n",
      "Terms with Assamese translations\n",
      "Terms with Azerbaijani translations\n",
      "Terms with Bashkir translations\n",
      "Terms with Belarusian translations\n",
      "Terms with Bikol Central translations\n",
      "Terms with Burmese translations\n",
      "Terms with Catalan translations\n",
      "Terms with Chukchi translations\n",
      "Terms with Danish translations\n",
      "Terms with Egyptian Arabic translations\n",
      "Terms with Estonian translations\n",
      "Terms with French translations\n",
      "Terms with Galician translations\n",
      "Terms with German Low German translations\n",
      "Terms with Gothic translations\n",
      "Terms with Gujarati translations\n",
      "Terms with Hebrew translations\n",
      "Terms with Hindi translations\n",
      "Terms with Icelandic translations\n",
      "Terms with Indonesian translations\n",
      "Terms with Interlingua translations\n",
      "Terms with Istriot translations\n",
      "Terms with Japanese translations\n",
      "Terms with Kabuverdianu translations\n",
      "Terms with Korean translations\n",
      "Terms with Latvian translations\n",
      "Terms with Lezgi translations\n",
      "Terms with Lindu translations\n",
      "Terms with Lombard translations\n",
      "Terms with Lower Sorbian translations\n",
      "Terms with Macedonian translations\n",
      "Terms with Malayalam translations\n",
      "Terms with Mandarin translations\n",
      "Terms with Moroccan Arabic translations\n",
      "Terms with Ngazidja Comorian translations\n",
      "Terms with Northern Kurdish translations\n",
      "Terms with Occitan translations\n",
      "Terms with Old Irish translations\n",
      "Terms with Persian translations\n",
      "Terms with Plautdietsch translations\n",
      "Terms with Portuguese translations\n",
      "Terms with Romagnol translations\n",
      "Terms with Russian translations\n",
      "Terms with Scottish Gaelic translations\n",
      "Terms with Seychellois Creole translations\n",
      "Terms with Slovak translations\n",
      "Terms with Somali translations\n",
      "Terms with Spanish translations\n",
      "Terms with Swedish translations\n",
      "Terms with Tagalog translations\n",
      "Terms with Tarantino translations\n",
      "Terms with Telugu translations\n",
      "Terms with Turkish translations\n",
      "Terms with Ukrainian translations\n",
      "Terms with Uyghur translations\n",
      "Terms with Volapük translations\n",
      "Terms with Welsh translations\n",
      "Terms with Yiddish translations\n",
      "Terms with Abkhaz translations\n",
      "Terms with Albanian translations\n",
      "Terms with Ancient Greek translations\n",
      "Terms with Armenian translations\n",
      "Terms with Assamese translations\n",
      "Terms with Azerbaijani translations\n",
      "Terms with Bashkir translations\n",
      "Terms with Belarusian translations\n",
      "Terms with Bikol Central translations\n",
      "Terms with Burmese translations\n",
      "Terms with Catalan translations\n",
      "Terms with Chukchi translations\n",
      "Terms with Danish translations\n",
      "Terms with Egyptian Arabic translations\n",
      "Terms with Estonian translations\n",
      "Terms with French translations\n",
      "Terms with Galician translations\n",
      "Terms with German Low German translations\n",
      "Terms with Gothic translations\n",
      "Terms with Gujarati translations\n",
      "Terms with Hebrew translations\n",
      "Terms with Hindi translations\n",
      "Terms with Icelandic translations\n",
      "Terms with Indonesian translations\n",
      "Terms with Interlingua translations\n",
      "Terms with Istriot translations\n",
      "Terms with Japanese translations\n",
      "Terms with Kabuverdianu translations\n",
      "Terms with Korean translations\n",
      "Terms with Latvian translations\n",
      "Terms with Lezgi translations\n",
      "Terms with Lindu translations\n",
      "Terms with Lombard translations\n",
      "Terms with Lower Sorbian translations\n",
      "Terms with Macedonian translations\n",
      "Terms with Malayalam translations\n",
      "Terms with Mandarin translations\n",
      "Terms with Moroccan Arabic translations\n",
      "Terms with Ngazidja Comorian translations\n",
      "Terms with Northern Kurdish translations\n",
      "Terms with Occitan translations\n",
      "Terms with Old Irish translations\n",
      "Terms with Persian translations\n",
      "Terms with Plautdietsch translations\n",
      "Terms with Portuguese translations\n",
      "Terms with Romagnol translations\n",
      "Terms with Russian translations\n",
      "Terms with Scottish Gaelic translations\n",
      "Terms with Seychellois Creole translations\n",
      "Terms with Slovak translations\n",
      "Terms with Somali translations\n",
      "Terms with Spanish translations\n",
      "Terms with Swedish translations\n",
      "Terms with Tagalog translations\n",
      "Terms with Tarantino translations\n",
      "Terms with Telugu translations\n",
      "Terms with Turkish translations\n",
      "Terms with Ukrainian translations\n",
      "Terms with Uyghur translations\n",
      "Terms with Volapük translations\n",
      "Terms with Welsh translations\n",
      "Terms with Yiddish translations\n",
      "Terms with Afrikaans translations\n",
      "Terms with Arabic translations\n",
      "Terms with Belarusian translations\n",
      "Terms with Catalan translations\n",
      "Terms with Czech translations\n",
      "Terms with Egyptian Arabic translations\n",
      "Terms with Estonian translations\n",
      "Terms with French translations\n",
      "Terms with German translations\n",
      "Terms with Hebrew translations\n",
      "Terms with Hungarian translations\n",
      "Terms with Indonesian translations\n",
      "Terms with Irish translations\n",
      "Terms with Japanese translations\n",
      "Terms with Latin translations\n",
      "Terms with Lithuanian translations\n",
      "Terms with Malayalam translations\n",
      "Terms with Maori translations\n",
      "Terms with Norwegian translations\n",
      "Terms with Old English translations\n",
      "Terms with Persian translations\n",
      "Terms with Portuguese translations\n",
      "Terms with Russian translations\n",
      "Terms with Serbo-Croatian translations\n",
      "Terms with Slovene translations\n",
      "Terms with Swedish translations\n",
      "Terms with Thai translations\n",
      "Terms with Ukrainian translations\n",
      "Terms with Vietnamese translations\n",
      "Terms with Yiddish translations\n",
      "Terms with Finnish translations\n",
      "Terms with Irish translations\n",
      "Terms with Arabic translations\n",
      "Terms with Bulgarian translations\n",
      "Terms with Czech translations\n",
      "Terms with Dzongkha translations\n",
      "Terms with Faroese translations\n",
      "Terms with French translations\n",
      "Terms with Georgian translations\n",
      "Terms with Greek translations\n",
      "Terms with Hindi translations\n",
      "Terms with Icelandic translations\n",
      "Terms with Indonesian translations\n",
      "Terms with Italian translations\n",
      "Terms with Khmer translations\n",
      "Terms with Latvian translations\n",
      "Terms with Macedonian translations\n",
      "Terms with Mandarin translations\n",
      "Terms with Norwegian Bokmål translations\n",
      "Terms with Polish translations\n",
      "Terms with Romanian translations\n",
      "Terms with Scottish Gaelic translations\n",
      "Terms with Sinhalese translations\n",
      "Terms with Spanish translations\n",
      "Terms with Tagalog translations\n",
      "Terms with Tibetan translations\n",
      "Terms with Ukrainian translations\n",
      "Terms with Acehnese translations\n",
      "Terms with Albanian translations\n",
      "Terms with Amharic translations\n",
      "Terms with Arabic translations\n",
      "Terms with Armenian translations\n",
      "Terms with Azerbaijani translations\n",
      "Terms with Baluchi translations\n",
      "Terms with Basque translations\n",
      "Terms with Bengali translations\n",
      "Terms with Brahui translations\n",
      "Terms with Bulgarian translations\n",
      "Terms with Cantonese translations\n",
      "Terms with Catalan translations\n",
      "Terms with Chuvash translations\n",
      "Terms with Czech translations\n",
      "Terms with Erzya translations\n",
      "Terms with Estonian translations\n",
      "Terms with Finnish translations\n",
      "Terms with Friulian translations\n",
      "Terms with Georgian translations\n",
      "Terms with Gilaki translations\n",
      "Terms with Gurani translations\n",
      "Terms with Hebrew translations\n",
      "Terms with Hungarian translations\n",
      "Terms with Ido translations\n",
      "Terms with Interlingua translations\n",
      "Terms with Italian translations\n",
      "Terms with Kannada translations\n",
      "Terms with Kazakh translations\n",
      "Terms with Korean translations\n",
      "Terms with Ladin translations\n",
      "Terms with Laki translations\n",
      "Terms with Latin translations\n",
      "Terms with Limburgish translations\n",
      "Terms with Livvi translations\n",
      "Terms with Lower Sorbian translations\n",
      "Terms with Macedonian translations\n",
      "Terms with Malayalam translations\n",
      "Terms with Maori translations\n",
      "Terms with Mongolian translations\n",
      "Terms with Nahuatl translations\n",
      "Terms with Navajo translations\n",
      "Terms with Northern Kurdish translations\n",
      "Terms with Northern Sami translations\n",
      "Terms with Norwegian Bokmål translations\n",
      "Terms with Occitan translations\n",
      "Terms with Ossetian translations\n",
      "Terms with Papiamentu translations\n",
      "Terms with Persian translations\n",
      "Terms with Polish translations\n",
      "Terms with Punjabi translations\n",
      "Terms with Russian translations\n",
      "Terms with Sardinian translations\n",
      "Terms with Slovak translations\n",
      "Terms with Southern Kurdish translations\n",
      "Terms with Swahili translations\n",
      "Terms with Tagalog translations\n",
      "Terms with Tatar translations\n",
      "Terms with Thai translations\n",
      "Terms with Tigrinya translations\n",
      "Terms with Turkmen translations\n",
      "Terms with Upper Sorbian translations\n",
      "Terms with Uyghur translations\n",
      "Terms with Venetan translations\n",
      "Terms with Volapük translations\n",
      "Terms with West Frisian translations\n",
      "Terms with Yiddish translations\n",
      "Terms with Zealandic translations\n",
      "Terms with Albanian translations\n",
      "Terms with Breton translations\n",
      "Terms with Czech translations\n",
      "Terms with Estonian translations\n",
      "Terms with French translations\n",
      "Terms with Interlingua translations\n",
      "Terms with Lithuanian translations\n",
      "Terms with Norwegian Nynorsk translations\n",
      "Terms with Persian translations\n",
      "Terms with Portuguese translations\n",
      "Terms with Russian translations\n",
      "Terms with Spanish translations\n",
      "Terms with Turkish translations\n",
      "Terms with Yoruba translations\n",
      "Terms with Abkhaz translations\n",
      "Terms with Adyghe translations\n",
      "Terms with Afrikaans translations\n",
      "Terms with Ainu translations\n",
      "Terms with Akkadian translations\n",
      "Terms with Alemannic German translations\n",
      "Terms with Algerian Arabic translations\n",
      "Terms with Ambonese Malay translations\n",
      "Terms with Amis translations\n",
      "Terms with Andi translations\n",
      "Terms with Aramaic translations\n",
      "Terms with Aromanian translations\n",
      "Terms with Asturian translations\n",
      "Terms with Aymara translations\n",
      "Terms with Bakung translations\n",
      "Terms with Baluchi translations\n",
      "Terms with Bandjalang translations\n",
      "Terms with Basque translations\n",
      "Terms with Bavarian translations\n",
      "Terms with Belarusian translations\n",
      "Terms with Bhojpuri translations\n",
      "Terms with Bislama translations\n",
      "Terms with Blin translations\n",
      "Terms with Bouyei translations\n",
      "Terms with Brunei Malay translations\n",
      "Terms with Buginese translations\n",
      "Terms with Burmese translations\n",
      "Terms with Cantonese translations\n",
      "Terms with Catalan translations\n",
      "Terms with Central Atlas Tamazight translations\n",
      "Terms with Central Kurdish translations\n",
      "Terms with Chagatai translations\n",
      "Terms with Chechen translations\n",
      "Terms with Cherokee translations\n",
      "Terms with Chichewa translations\n",
      "Terms with Chukchi translations\n",
      "Terms with Chuvash translations\n",
      "Terms with Classical Syriac translations\n",
      "Terms with Cornish translations\n",
      "Terms with Cree translations\n",
      "Terms with Czech translations\n",
      "Terms with Dargwa translations\n",
      "Terms with Dhuwal translations\n",
      "Terms with Dungan translations\n",
      "Terms with Dzongkha translations\n",
      "Terms with Eastern Cham translations\n",
      "Terms with Eastern Min translations\n",
      "Terms with Egyptian translations\n",
      "Terms with Erzya translations\n",
      "Terms with Esperanto translations\n",
      "Terms with Even translations\n",
      "Terms with Farefare translations\n",
      "Terms with Fijian translations\n",
      "Terms with Fon translations\n",
      "Terms with French translations\n",
      "Terms with Fula translations\n",
      "Terms with Gamilaraay translations\n",
      "Terms with Georgian translations\n",
      "Terms with German translations\n",
      "Terms with Godoberi translations\n",
      "Terms with Gooniyandi translations\n",
      "Terms with Greenlandic translations\n",
      "Terms with Gujarati translations\n",
      "Terms with Haitian Creole translations\n",
      "Terms with Halkomelem translations\n",
      "Terms with Hawaiian translations\n",
      "Terms with Higaonon translations\n",
      "Terms with Hiligaynon translations\n",
      "Terms with Hlai translations\n",
      "Terms with Hopi translations\n",
      "Terms with Hunsrik translations\n",
      "Terms with Icelandic translations\n",
      "Terms with Ilocano translations\n",
      "Terms with Indonesian translations\n",
      "Terms with Interlingua translations\n",
      "Terms with Iraqi Arabic translations\n",
      "Terms with Italian translations\n",
      "Terms with Jarai translations\n",
      "Terms with Jeju translations\n",
      "Terms with Juba Arabic translations\n",
      "Terms with Kabardian translations\n",
      "Terms with Kalasha translations\n",
      "Terms with Kambaata translations\n",
      "Terms with Kapampangan translations\n",
      "Terms with Karakalpak translations\n",
      "Terms with Kashubian translations\n",
      "Terms with Khakas translations\n",
      "Terms with Khiamniungan Naga translations\n",
      "Terms with Khoekhoe translations\n",
      "Terms with Kildin Sami translations\n",
      "Terms with Kimbundu translations\n",
      "Terms with Konkani translations\n",
      "Terms with Ktunaxa translations\n",
      "Terms with Kyrgyz translations\n",
      "Terms with Ladin translations\n",
      "Terms with Lak translations\n",
      "Terms with Lao translations\n",
      "Terms with Latin translations\n",
      "Terms with Laz translations\n",
      "Terms with Libyan Arabic translations\n",
      "Terms with Lipan translations\n",
      "Terms with Lombard translations\n",
      "Terms with Lower Sorbian translations\n",
      "Terms with Luxembourgish translations\n",
      "Terms with Macedonian translations\n",
      "Terms with Malagasy translations\n",
      "Terms with Malayalam translations\n",
      "Terms with Maltese translations\n",
      "Terms with Mandarin translations\n",
      "Terms with Maori translations\n",
      "Terms with Maranao translations\n",
      "Terms with Maricopa translations\n",
      "Terms with Mauritian Creole translations\n",
      "Terms with Mbyá Guaraní translations\n",
      "Terms with Meänkieli translations\n",
      "Terms with Middle French translations\n",
      "Terms with Miyako translations\n",
      "Terms with Mon translations\n",
      "Terms with Montagnais translations\n",
      "Terms with Moroccan Arabic translations\n",
      "Terms with Mòcheno translations\n",
      "Terms with Navajo translations\n",
      "Terms with Nepali translations\n",
      "Terms with Ngazidja Comorian translations\n",
      "Terms with Nobiin translations\n",
      "Terms with Norman translations\n",
      "Terms with North Levantine Arabic translations\n",
      "Terms with Northern Kurdish translations\n",
      "Terms with Northern Sami translations\n",
      "Terms with Norwegian Bokmål translations\n",
      "Terms with Nuosu translations\n",
      "Terms with Occitan translations\n",
      "Terms with Ojibwe translations\n",
      "Terms with Old Church Slavonic translations\n",
      "Terms with Old English translations\n",
      "Terms with Old High German translations\n",
      "Terms with Old Norse translations\n",
      "Terms with Orok translations\n",
      "Terms with Oroqen translations\n",
      "Terms with Ottoman Turkish translations\n",
      "Terms with Paipai translations\n",
      "Terms with Panamint translations\n",
      "Terms with Persian translations\n",
      "Terms with Picard translations\n",
      "Terms with Pipil translations\n",
      "Terms with Plautdietsch translations\n",
      "Terms with Portuguese translations\n",
      "Terms with Quechua translations\n",
      "Terms with Romagnol translations\n",
      "Terms with Romanian translations\n",
      "Terms with Russian translations\n",
      "Terms with Samoan translations\n",
      "Terms with Sanskrit translations\n",
      "Terms with Saraiki translations\n",
      "Terms with Sardinian translations\n",
      "Terms with Sebop translations\n",
      "Terms with Semai translations\n",
      "Terms with Seri translations\n",
      "Terms with Shan translations\n",
      "Terms with Sicilian translations\n",
      "Terms with Sinhalese translations\n",
      "Terms with Slovak translations\n",
      "Terms with Somali translations\n",
      "Terms with Southern Altai translations\n",
      "Terms with Southern Sami translations\n",
      "Terms with Sranan Tongo translations\n",
      "Terms with Sundanese translations\n",
      "Terms with Swahili translations\n",
      "Terms with Sylheti translations\n",
      "Terms with Tagal Murut translations\n",
      "Terms with Tahitian translations\n",
      "Terms with Tai Nüa translations\n",
      "Terms with Talysh translations\n",
      "Terms with Taos translations\n",
      "Terms with Tat translations\n",
      "Terms with Telugu translations\n",
      "Terms with Tetum translations\n",
      "Terms with Tibetan translations\n",
      "Terms with Tigrinya translations\n",
      "Terms with Tongan translations\n",
      "Terms with Torres Strait Creole translations\n",
      "Terms with Tulu translations\n",
      "Terms with Turkmen translations\n",
      "Terms with Tuvan translations\n",
      "Terms with Tz'utujil translations\n",
      "Terms with Uab Meto translations\n",
      "Terms with Ukrainian translations\n",
      "Terms with Unami translations\n",
      "Terms with Urdu translations\n",
      "Terms with Uzbek translations\n",
      "Terms with Vietnamese translations\n",
      "Terms with Volapük translations\n",
      "Terms with Waray-Waray translations\n",
      "Terms with West Coast Bajau translations\n",
      "Terms with Western Apache translations\n",
      "Terms with White Hmong translations\n",
      "Terms with Woiwurrung translations\n",
      "Terms with Wu translations\n",
      "Terms with Yaeyama translations\n",
      "Terms with Yakut translations\n",
      "Terms with Yindjibarndi translations\n",
      "Terms with Yoruba translations\n",
      "Terms with Zealandic translations\n",
      "Terms with Zulu translations\n",
      "Terms with Afrikaans translations\n",
      "Terms with Danish translations\n",
      "Terms with Finnish translations\n",
      "Terms with Italian translations\n",
      "Terms with Norwegian translations\n",
      "Terms with Portuguese translations\n",
      "Terms with Sedang translations\n",
      "Terms with Urdu translations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "temp_output_file = Path(current_save_path, \"test_output.jsonl\")\n",
    "batch_size = 10\n",
    "error_lines = []\n",
    "from pprint import pp, pprint\n",
    "\n",
    "with open(temp_output_file, \"w+\", encoding='utf-8') as output_file:\n",
    "    with open(ERAW_FILE, \"r\", encoding='utf-8') as f:\n",
    "        batch_list = []\n",
    "        for i, line in tqdm(enumerate(f), total=total_en_lines):\n",
    "            try:\n",
    "                if not keep_obj_before_load(line):\n",
    "                    continue\n",
    "               \n",
    "                line = filter_line(line)\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                \n",
    "                    if not keep_obj(obj):\n",
    "                        continue\n",
    "                    \n",
    "                    filter_obj(obj)\n",
    "                    batch_list.append(obj)\n",
    "\n",
    "                    if i % batch_size == 0 and i > 0:\n",
    "                        for obj in batch_list:\n",
    "                            json.dump(obj, output_file, ensure_ascii=False)\n",
    "                            output_file.write(\"\\n\")\n",
    "                        batch_list = []\n",
    "                        break\n",
    "                except Exception as e1:\n",
    "                    error_lines.append((i, line))\n",
    "                    print(f\"Error parsing JSON on line {i} \", e1)\n",
    "            except Exception as e:\n",
    "\n",
    "                print(f.tell())\n",
    "                raise e        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25712dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error lines: 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[286]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal error lines: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(error_lines)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m display(\u001b[43merror_lines\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(f\"Total error lines: {len(error_lines)}\")\n",
    "display(error_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2569c6",
   "metadata": {},
   "source": [
    "### Split Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7886c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_jsonl_by_lines(f, output_dir, lines_per_chunk=1_000_000):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    part = 1\n",
    "    line_count = 0\n",
    "    outfile = open(os.path.join(output_dir, f\"chunk_{part:03}.jsonl\"), \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    with open(f, \"r\", encoding=\"utf-8\", errors=\"ignore\") as infile:\n",
    "        for line in tqdm(infile, total=count_lines_with_progress):\n",
    "            if line_count >= lines_per_chunk:\n",
    "                outfile.close()\n",
    "                part += 1\n",
    "                line_count = 0\n",
    "                outfile = open(os.path.join(output_dir, f\"chunk_{part:03}.jsonl\"), \"w\", encoding=\"utf-8\")\n",
    "            outfile.write(line)\n",
    "            line_count += 1\n",
    "\n",
    "    outfile.close()\n",
    "    print(f\"✅ Done. Created {part} chunks in '{output_dir}/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe11af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, os\n",
    "\n",
    "def split_and_fix_jsonl(input_path, total_lines, lines_per_chunk=1_000_000, output_dir=\"chunks\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    part = 1\n",
    "    count = 0\n",
    "    out = open(os.path.join(output_dir, f\"chunk_{part:03}.jsonl\"), \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in tqdm(f, total=total_lines):\n",
    "            # split concatenated JSON objects\n",
    "            parts = re.split(r'(?=\\{\"senses\":)', line)\n",
    "            for p in parts:\n",
    "                p = p.strip()\n",
    "                if not p:\n",
    "                    continue\n",
    "                try:\n",
    "                    json.loads(p)  # validate\n",
    "                    out.write(p + \"\\n\")\n",
    "                    count += 1\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "                if count >= lines_per_chunk:\n",
    "                    out.close()\n",
    "                    part += 1\n",
    "                    count = 0\n",
    "                    out = open(os.path.join(output_dir, f\"chunk_{part:03}.jsonl\"), \"w\", encoding=\"utf-8\")\n",
    "    out.close()\n",
    "    print(f\"✅ Finished writing {part} chunk files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_and_fix_jsonl(ERAW_FILE, output_dir=chunks_dir, total_lines=total_en_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909bd325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "\n",
    "def split_large_json_objects(input_path, output_dir=\"chunks\", chunk_size=500_000):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    part = 1\n",
    "    count = 0\n",
    "    out = open(os.path.join(output_dir, f\"chunk_{part:03}.jsonl\"), \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        parser = ijson.items(f, \"\", multiple_values=True)  # accept concatenated JSON objects\n",
    "        for obj in parser:\n",
    "            json.dump(obj, out, ensure_ascii=False)\n",
    "            out.write(\"\\n\")\n",
    "            count += 1\n",
    "\n",
    "            if count >= chunk_size:\n",
    "                out.close()\n",
    "                lines_start = part*chunk_size - chunk_size\n",
    "                lines_end = lines_start + chunk_size\n",
    "                print(f\"Part: {part} Lines: {lines_start}-{lines_end} complete\")\n",
    "                part += 1\n",
    "                count = 0\n",
    "                out = open(os.path.join(output_dir, f\"chunk_{part:03}.jsonl\"), \"w\", encoding=\"utf-8\")\n",
    "    \n",
    "    out.close()\n",
    "    print(f\"✅ Finished splitting into {part} chunks in '{output_dir}/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d69953",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks2 = Path(current_save_path, 'en', 'chunks2')\n",
    "#split_large_json_objects(ERAW_FILE, output_dir=chunks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3362f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickletools import read_bytes1\n",
    "\n",
    "\n",
    "def get_longest_line(file_path, chunk_size=1024 * 1024):\n",
    "    total_size = os.path.getsize(file_path)\n",
    "    lines = 0\n",
    "    longest_line = 0\n",
    "    longest_lines = []\n",
    "    num_chunks_longest = 0\n",
    "    current_chunks_count = 1\n",
    "    current_line = 0\n",
    "    lowest_chunks_per_line = 1000000\n",
    "    with open(file_path, 'rb') as f, tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Counting Lines\") as pbar:\n",
    "        current_chunk = b''\n",
    "        last_chunk = b''\n",
    "        while chunk := f.read(chunk_size):\n",
    "            chunk_count = chunk.count(b'\\n')\n",
    "            if chunk_count < lowest_chunks_per_line:\n",
    "                lowest_chunks_per_line = chunk_count\n",
    "                \n",
    "                longest_lines.append(((1, chunk_count), chunk))\n",
    "            else:\n",
    "                if chunk_count == 0:\n",
    "                    if current_chunks_count == 1:\n",
    "                        current_chunk = last_chunk\n",
    "                    current_chunk += chunk\n",
    "                    current_chunks_count += 1\n",
    "                else:\n",
    "                    last_chunk = chunk\n",
    "                    if current_chunks_count > num_chunks_longest:\n",
    "                        longest_line = current_chunk\n",
    "\n",
    "                        num_chunks_longest =  current_chunks_count\n",
    "                        longest_lines.append((current_chunks_count, current_chunk))\n",
    "                        current_chunks_count = 1\n",
    "\n",
    "            lines += chunk_count\n",
    "            pbar.update(len(chunk))\n",
    "            \n",
    "    return lines, longest_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45106aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_en_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dbaace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "last_lines = deque(maxlen=5_000_000)\n",
    "with open(ERAW_FILE, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        last_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc3167",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_5_million_lines_file = Path(WIKT_PREPROCESSING_DIR, '07-11-25', 'en', 'last_5_million.jsonl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5f17d",
   "metadata": {},
   "source": [
    "### Get Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadac13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c, long_lines = get_longest_line(ERAW_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7be83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(long_lines))\n",
    "print(long_lines[-1][0])\n",
    "print(long_lines[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78122f24",
   "metadata": {},
   "source": [
    "### json_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b6cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_to_file(json_lines, output_dir, file_name='', overwrite=True, suppress_print=False):\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=overwrite)\n",
    "    except:\n",
    "        print('error or overwrite not allowed')\n",
    "        return False\n",
    "\n",
    "    if not file_name:\n",
    "        file_name = \"out.jsonl\"\n",
    "    with open(Path(output_dir, file_name), 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_lines, f, ensure_ascii=False, indent=1)\n",
    "    if not suppress_print:\n",
    "        print(file_name, ' saved')\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3386eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_files_to_dir(json_files_contents_list, output_dir, json_file_names_list=[], out_format='jsonl', overwrite=True, file_prefix='', suppress_print=False):\n",
    "    if not json_file_names_list:\n",
    "        if not file_prefix:\n",
    "            file_prefix = 'out'\n",
    "        json_file_names_list = [f\"{file_prefix}_{i}.{out_format}\" for i in range(len(json_files_contents_list))] \n",
    "    else:\n",
    "        json_file_list = [f\"{x}.{out_format}\" for x in range(len(json_files_contents_list))]\n",
    "\n",
    "    if len(json_files_contents_list) != len(json_file_list):\n",
    "        print('Length of names does not match')\n",
    "        return   \n",
    "\n",
    "    try:\n",
    "            os.makedirs(output_dir, exist_ok=overwrite)\n",
    "            files_in_dir = os.listdir(output_dir)\n",
    "            for i in len(range(json_files_contents_list)):\n",
    "                contents = json_files_contents_list[i]\n",
    "                name = json_file_list[i]\n",
    "                save_json_to_file(contents, output_dir, name, overwrite=overwrite, suppress_print=suppress_print)\n",
    "                \n",
    "                \n",
    "    except Exception as e:\n",
    "        print('error with write')\n",
    "        \n",
    "    \n",
    "        \n",
    "    # else:\n",
    "    #     try:\n",
    "    #         os.makedirs(output_dir, exist_ok=overwrite)\n",
    "    #     except Exception as e:\n",
    "    #         print(e)\n",
    "    #         if e == OSError and overwrite==False:\n",
    "    #             files_in_dir = os.listdir(output_dir)\n",
    "\n",
    "    #             matching_files = [x for x in json_file_names_list if x in files_in_dir]\n",
    "    #             num_matches = len(matching_files)\n",
    "    #             user_input = input(f'Directory exists and {num_matches} files match: 1: quit: \\n 2: overwrite whole directory: \\n, 3: pick overwrite for each file: \\n')\n",
    "    #             for file in matching_files:\n",
    "    #                 if user_input == 1:\n",
    "    #                         return\n",
    "    #                 elif user_input == 3:\n",
    "    #                     print('Overwrite (y/n)?')\n",
    "    #                     for i, file in enumerate(matching_files):\n",
    "    #                         file_choice = input(f'{file}: ')\n",
    "    #                         if file_choice == 'y':\n",
    "    #                             print('not implemented')\n",
    "    #                             break\n",
    "    #                 elif user_input == 2:\n",
    "    #                     overwrite = True\n",
    "                    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d2352",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ENR_lines = []\n",
    "\n",
    "counts_index = 3\n",
    "first_chunk = Path(chunks2, 'chunk_001.jsonl')\n",
    "all_chunks = os.listdir(chunks2)\n",
    "en_rough_save_path = Path(current_save_path, 'en', 'rough_line_files2')\n",
    "ENR_files = []\n",
    "EER_files = []\n",
    "other_lines_dutch_files = []\n",
    "remaining_lines_files = []\n",
    "lines_count = 0\n",
    "appended_lines = 0\n",
    "english_dutch_lines = []\n",
    "ENR_lines = []\n",
    "EER_lines = []\n",
    "no_dutch_lines = []\n",
    "other_lines_with_dutch_and_en_translations = []\n",
    "\n",
    "save_limit = 100000\n",
    "#file = all_chunks[0]\n",
    "for file in all_chunks:\n",
    "\n",
    "    print(\"Processing file: \", file)\n",
    "    \n",
    "    file_path = Path(chunks2, file)\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8',errors='ignore') as f:\n",
    "        \n",
    "        lines = f.readlines()\n",
    "        for line in tqdm(lines):\n",
    "            has_dutch = False\n",
    "            lines_count += 1\n",
    "            if not has_cjk_or_arabic_fast(line):\n",
    "                if line.find('\"lang\": \"Dutch\"' ) != -1:\n",
    "                    has_dutch = True\n",
    "                elif line.find('\"lang_code\": \"nl\"' ) != -1:\n",
    "                    has_dutch = True\n",
    "\n",
    "                \n",
    "\n",
    "                if has_dutch:\n",
    "                    loaded = json.loads(line)\n",
    "                    if not loaded:\n",
    "                        continue\n",
    "                    lang_code = loaded.get(\"lang_code\", None)\n",
    "                    if lang_code:\n",
    "                        if lang_code == 'en':\n",
    "                            EER_lines.append(loaded)\n",
    "                        elif lang_code == 'nl':\n",
    "                            ENR_lines.append(loaded)\n",
    "                        else:\n",
    "                            english_dutch_lines.append(loaded)\n",
    "                        appended_lines += 1\n",
    "                    has_dutch = False\n",
    "                    \n",
    "                else:\n",
    "                    if not has_cjk_or_arabic_fast(line[50:100]):\n",
    "                        loaded = json.loads(line)\n",
    "                        lang_code = loaded.get(\"lang_code\", None)\n",
    "                        if lang_code:\n",
    "                            if lang_code == 'en':\n",
    "                                EER_lines.append(loaded)\n",
    "                            else:\n",
    "                                no_dutch_lines.append(line)\n",
    "                            appended_lines += 1\n",
    "            else:\n",
    "                continue\n",
    "            if len(EER_lines)%save_limit == 0 and len(EER_lines) !=0:\n",
    "                save_json_to_file(EER_lines, en_rough_save_path, f\"EER_rough_{len(EER_files)}.jsonl\", suppress_print=True)\n",
    "                EER_files.append(f\"EER_rough_{len(EER_files)}.jsonl\")\n",
    "                EER_lines = []\n",
    "\n",
    "            if len(ENR_lines)%save_limit == 0 and len(ENR_lines) != 0:\n",
    "                save_json_to_file(ENR_lines, en_rough_save_path, f\"ENR_rough_{len(ENR_files)}.jsonl\", suppress_print=True)\n",
    "                ENR_files.append(f\"ENR_rough_{len(ENR_files)}.jsonl\")\n",
    "                ENR_lines = []\n",
    "            if len(english_dutch_lines)%save_limit == 0 and len(english_dutch_lines) > 0:\n",
    "            \n",
    "                save_json_to_file(english_dutch_lines, en_rough_save_path, f\"other_dutch_lines_{len(other_lines_dutch_files)}.jsonl\", suppress_print=True)\n",
    "                other_lines_dutch_files.append(f\"other_dutch_lines_{len(other_lines_dutch_files)}.jsonl\")\n",
    "                english_dutch_lines = []\n",
    "            if len(no_dutch_lines)%200000 == 0 and len(no_dutch_lines) > 0:\n",
    "                save_json_to_file(no_dutch_lines, en_rough_save_path, f\"no_dutch_lines_{len(remaining_lines_files)}.jsonl\", suppress_print=True)\n",
    "                remaining_lines_files.append(f\"no_dutch_lines_{len(remaining_lines_files)}.jsonl\")\n",
    "                no_dutch_lines = []\n",
    "    print('Now appended lines: ', appended_lines, \" of \", lines_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a05b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json_to_file(EER_lines, en_rough_save_path, f\"EER_rough_{len(EER_files)}.jsonl\", suppress_print=True)\n",
    "EER_files.append(f\"EER_rough_{len(EER_files)}.jsonl\")\n",
    "EER_lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2de534",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json_to_file(ENR_lines, en_rough_save_path, f\"ENR_rough_{len(ENR_files)}.jsonl\", suppress_print=True)\n",
    "ENR_files.append(f\"ENR_rough_{len(ENR_files)}.jsonl\")\n",
    "ENR_lines = []\n",
    "save_json_to_file(english_dutch_lines, en_rough_save_path, f\"other_dutch_lines_{len(other_lines_dutch_files)}.jsonl\", suppress_print=True)\n",
    "other_lines_dutch_files.append(f\"other_dutch_lines_{len(other_lines_dutch_files)}.jsonl\")\n",
    "english_dutch_lines = []\n",
    "save_json_to_file(no_dutch_lines, en_rough_save_path, f\"no_dutch_lines_{len(remaining_lines_files)}.jsonl\", suppress_print=True)\n",
    "remaining_lines_files.append(f\"no_dutch_lines_{len(remaining_lines_files)}.jsonl\")\n",
    "no_dutch_lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(EER_files))\n",
    "print(len(ENR_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def concat_lang_strip_files(save_folder, source_folder, prefix='EER', lang='en', translation_lang='nl'):\n",
    "    \"\"\"Load all JSONL files with `prefix` in filename from `source_folder`,\n",
    "    remove all translations except those matching `translation_lang`,\n",
    "    and return the cleaned list of objects.\"\"\"\n",
    "    \n",
    "    new_lines = []\n",
    "    files = [f for f in os.listdir(source_folder) if prefix in f]\n",
    "    \n",
    "    for file in files:\n",
    "        path = Path(source_folder, file)\n",
    "        print(f\"Processing {path} ...\")\n",
    "        \n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            for line_no, line in tqdm(enumerate(f, 1)):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    loaded = json.loads(line)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"⚠️ Skipping bad JSON (line {line_no} in {file}): {e}\")\n",
    "                    continue\n",
    "\n",
    "                if \"translations\" in loaded and isinstance(loaded[\"translations\"], list):\n",
    "                    translations = loaded[\"translations\"]\n",
    "                    translation_list = [\n",
    "                        t for t in translations if isinstance(t, dict) and t.get(\"lang_code\") == translation_lang]\n",
    "\n",
    "                    # Only keep translations if we found Dutch ones\n",
    "                    if translation_list:\n",
    "                        loaded[\"translations\"] = translation_list\n",
    "                    else:\n",
    "                        loaded.pop(\"translations\", None)\n",
    "\n",
    "                new_lines.append(loaded)\n",
    "    \n",
    "    print(f\"✅ Processed {len(new_lines):,} total objects.\")\n",
    "    return new_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d52c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def concat_lang_strip_files(prefix, save_folder, source_folder, lang='en', translation_lang='nl'):\n",
    "#     files = [f for f in os.listdir(source_folder) if prefix in f]\n",
    "#     new_lines = []\n",
    "#     for file in files:\n",
    "#         with open(Path(source_folder, file), 'r', encoding='utf-8') as f:\n",
    "#             for line in f:\n",
    "#                 loaded = json.loads(line)\n",
    "#                 if line.find('translations') != -1:\n",
    "                    \n",
    "#                     translations = loaded.get(translations, None)\n",
    "#                     translation_list = []\n",
    "#                     if translations:\n",
    "#                         if translations.find('\"lang_code\": \"{translation_lang}\"') != -1:\n",
    "#                             for t in translations:\n",
    "#                                 l = t.get('lang_code', None)\n",
    "#                                 if l == translation_lang:\n",
    "#                                     translation_list.append(t)\n",
    "#                         loaded.pop(translations)\n",
    "#                         if translation_list:\n",
    "#                             loaded['translations'] = translation_list\n",
    "\n",
    "#                 new_lines.append(loaded)\n",
    "\n",
    "#     return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd15fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_rough_save_path = Path(current_save_path, 'en', 'rough_line_files2')\n",
    "new_EER = concat_lang_strip_files(source_folder=en_rough_save_path, prefix='EER', save_folder=en_rough_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f468d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ENR_lines = []\n",
    "\n",
    "counts_index = 3\n",
    "first_chunk = Path(chunks2, 'chunk_001.jsonl')\n",
    "all_chunks = os.listdir(chunks2)\n",
    "en_rough_save_path = Path(current_save_path, 'en', 'rough_line_files')\n",
    "ENR_files = []\n",
    "EER_files = []\n",
    "other_lines_dutch_files = []\n",
    "remaining_lines_files = []\n",
    "lines_count = 0\n",
    "appended_lines = 0\n",
    "english_dutch_lines = []\n",
    "ENR_lines = []\n",
    "EER_lines = []\n",
    "no_dutch_lines = []\n",
    "other_lines_with_dutch_and_en_translations = []\n",
    "save_limit = 100000\n",
    "file = all_chunks[0]\n",
    "# for file in all_chunks:\n",
    "if file == all_chunks[0]:\n",
    "    print(\"Processing file: \", file)\n",
    "    \n",
    "    file_path = Path(chunks2, file)\n",
    "    has_dutch = False\n",
    "    with open(file_path, 'r', encoding='utf-8',errors='ignore') as f:\n",
    "        \n",
    "        lines = f.readlines()\n",
    "        for line in tqdm(lines):\n",
    "            lines_count += 1\n",
    "            if not has_cjk_or_arabic_fast(line):\n",
    "                if line.find('\"lang\": \"Dutch\"' ) != -1:\n",
    "                    has_dutch = True\n",
    "                elif line.find('\"lang_code\": \"nl\"' ) != -1:\n",
    "                    has_dutch = True\n",
    "\n",
    "                \n",
    "\n",
    "                if has_dutch:\n",
    "                    loaded = json.loads(line)\n",
    "                    if not loaded:\n",
    "                        continue\n",
    "                    lang_code = loaded.get(\"lang_code\", None)\n",
    "                    if lang_code:\n",
    "                        if lang_code == 'en':\n",
    "                            EER_lines.append(loaded)\n",
    "                        elif lang_code == 'nl':\n",
    "                            ENR_lines.append(loaded)\n",
    "                        else:\n",
    "                            english_dutch_lines.append(loaded)\n",
    "                        appended_lines += 1\n",
    "                    has_dutch = False\n",
    "                    \n",
    "                else:\n",
    "                    if not has_cjk_or_arabic_fast(line[50:100]):\n",
    "                        loaded = json.loads(line)\n",
    "                        lang_code = loaded.get(\"lang_code\", None)\n",
    "                        if lang_code:\n",
    "                            if lang_code == 'en':\n",
    "                                EER_lines.append(loaded)\n",
    "                            else:\n",
    "                                no_dutch_lines.append(line)\n",
    "                            appended_lines += 1\n",
    "            else:\n",
    "                continue\n",
    "    if len(EER_lines)%save_limit == 0:\n",
    "        save_json_to_file(EER_lines, en_rough_save_path, f\"EER_rough_{file[-8:]}\", suppress_print=True)\n",
    "    if len(ENR_lines)%save_limit == 0:\n",
    "        save_json_to_file(ENR_lines, en_rough_save_path, f\"ENR_rough_{file[-8:]}\", suppress_print=True)\n",
    "    if len(english_dutch_lines)%save_limit == 0:\n",
    "        save_json_to_file(english_dutch_lines, en_rough_save_path, f\"other_dutch_lines_{file[-8:]}\", suppress_print=True)\n",
    "    if len(no_dutch_lines)%save_limit == 0:\n",
    "        save_json_to_file(no_dutch_lines, en_rough_save_path, f\"no_dutch_lines_{file[-8:]}\", suppress_print=True)\n",
    "    print('Now appended lines: ', appended_lines, \" of \", lines_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc2979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_ENR_lines))\n",
    "total = 0\n",
    "for line_list in all_ENR_lines:\n",
    "    total += len(line_list)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79f561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = 0\n",
    "\n",
    "for file in os.listdir(en_rough_save_path, 'rough_line_files'):\n",
    " \n",
    "    eer_no_dutch = []\n",
    "    if 'no_dutch' in file:\n",
    "        with open(Path(en_rough_save_path, file)):\n",
    "        \n",
    "            lines = json.loads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18113d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "EER_files = []\n",
    "ENR_files = []\n",
    "translations_files = []\n",
    "lang_codes = ['en', 'nl']\n",
    "ENR_lines = []\n",
    "EER_lines = []\n",
    "remaining_lines = []\n",
    "stop_size = 100000\n",
    "ENR_count = 0\n",
    "EER_count = 0\n",
    "other_translations_count = 0\n",
    "remaining_lines_count = 0\n",
    "enr_file_num = 0\n",
    "eer_file_num = 0\n",
    "other_translation_file_num = 0\n",
    "remaining_lines_file_num = 0 \n",
    "lines_split_path = Path(current_save_path, 'lines_split')\n",
    "for file in os.listdir(chunks2):\n",
    "    \n",
    "    with open(Path(chunks2, file), 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in tqdm(lines):\n",
    "            loaded = json.loads(line)\n",
    "            lang_code = loaded.get('lang_code', None)\n",
    "            \n",
    "            if lang_code:\n",
    "                if lang_code == 'nl':\n",
    "                    ENR_count += 1\n",
    "                    ENR_lines.append(loaded)\n",
    "                    if len(ENR_lines)%stop_size == 0 and ENR_count != 0:\n",
    "                        save_json_to_file(ENR_lines, Path(lines_split_path, 'ENR'), f\"ENR_{enr_file_num}.jsonl\")\n",
    "                        enr_file_num += 1\n",
    "                        ENR_lines = []\n",
    "                elif lang_code == 'en':\n",
    "                    EER_count += 1\n",
    "                    EER_lines.append(loaded)\n",
    "                    if len(EER_lines)%stop_size == 0 and EER_count != 0:\n",
    "                        save_json_to_file(EER_lines, Path(lines_split_path, 'EER'), f\"EER_{eer_file_num}.jsonl\")\n",
    "                        eer_file_num += 1\n",
    "                        EER_lines = []\n",
    "                else:\n",
    "                    remaining_lines_count += 1\n",
    "                    remaining_lines.append(loaded)\n",
    "                    if len(remaining_lines)%stop_size == 0 and remaining_lines_count != 0:\n",
    "                        save_json_to_file(remaining_lines, Path(lines_split_path, 'remaining'), f\"remaining_{remaining_lines_file_num}.jsonl\")\n",
    "                        remaining_lines_file_num += 1\n",
    "                        remaining_lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e74e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from collections import Counter\n",
    "lines = []\n",
    "counts_index = 3\n",
    "\n",
    "with open(ERAW_FILE, 'r', encoding='utf-8',errors='ignore') as f:\n",
    "    line = f.readline()\n",
    "    print(len(line))\n",
    "    loaded = json.loads(line)\n",
    "    keys = loaded.keys()\n",
    "\n",
    "    lines.append(loaded)\n",
    "\n",
    "    d = get_subkeys(loaded)\n",
    "    d2 = get_subkeysV2(loaded)\n",
    "    display(len(d))\n",
    "    display(len(d2))\n",
    "    display(d)\n",
    "    print('-------------------------------------------------------------')\n",
    "    display(d2)\n",
    "\n",
    "    \n",
    "    #display(d2)\n",
    "    #display(d2)\n",
    "    # for k in keys:\n",
    "    #     if type(loaded[k]) == list:\n",
    "    #         subkeys = get_keys_from_list(loaded[k])\n",
    "    #         subkeys_list.append(subkeys)\n",
    "    #     print(subkeys)\n",
    "    # print(subkeys_list)\n",
    "    #structure_dict = get_nested_structure(loaded)\n",
    "    #display(structure_dict)\n",
    "    # for k in keys:\n",
    "    #     val = loaded[k]\n",
    "    #     type_tuples = make_structure_line_tuple(k, val)\n",
    "    #     types = type_tuples[counts_index]\n",
    "    #     if 'dict' in types.keys():\n",
    "            \n",
    "    #         print(types[3].keys())\n",
    "    #         print(types)\n",
    "        # counts = Counter()\n",
    "        # typecounts = Counter(type(x).__name__ for x in val)\n",
    "        # print(typecounts)\n",
    "    #print(loaded)\n",
    "\n",
    "    #structure = get_nested_structure(loaded)\n",
    "    #display(structure)\n",
    "    \n",
    "    # if loaded:\n",
    "    #     if isinstance(loaded, (dict, list)):\n",
    "    #         structure_dict = get_nested_structure(loaded)\n",
    "        \n",
    "\n",
    "    # for k in keys:\n",
    "    #     kl2 = []\n",
    "    #     val = loaded[k]\n",
    "    #     if isinstance(val, dict):\n",
    "    #         kl2 = loaded[k].keys()\n",
    "            \n",
    "    #         sl2 = []\n",
    "    #         for k2 in kl2:\n",
    "    #             v3 = get_nested_structure(k2)\n",
    "    #             if v3 != {} and v3 != []:\n",
    "    #                 sl2.append(v3)\n",
    "    #         structure_dict[k] = set(sl2)\n",
    "    #     elif isinstance(val, list):\n",
    "            \n",
    "    #         sl3 = []\n",
    "    #         try:\n",
    "    #             for i in val:\n",
    "    #                 v3 = get_nested_structure(i)\n",
    "    #                 if v3 != {}:\n",
    "    #                     sl3.append(v3)\n",
    "                        \n",
    "    #         except Exception as e:\n",
    "    #             print(k, i)\n",
    "    #             print(e)\n",
    "    #             continue\n",
    "    #         structure_dict[k] = sl3\n",
    "    # print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f527f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in structure_dict.items():\n",
    "    print(k)\n",
    "    display(v)\n",
    "    print('--------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EER_words = []\n",
    "# ENR_words = []\n",
    "en_words = []\n",
    "error_lines =[]\n",
    "\n",
    "\n",
    "\n",
    "count = 0\n",
    "with open(ERAW_FILE, 'r', encoding='utf-8',errors='ignore') as f:\n",
    "    for line in f:\n",
    "        line = f.readline()\n",
    "        line = line.strip()\n",
    "        \n",
    "        if count%100000 == 0:\n",
    "            print(count, ' - ', line)\n",
    "            \n",
    "        # count += 1    \n",
    "        # if not line:\n",
    "        #     error_lines.append(line)\n",
    "        #     continue\n",
    "        \n",
    "        posnl = line.find('\"lang_code\": \"nl\"')\n",
    "        posen = line.find('\"lang_code\": \"en\"')\n",
    "        if posnl == -1 and posen == -1:\n",
    "            error_lines.append(line)\n",
    "            continue\n",
    "        en_words.append(line)\n",
    "        # loaded_line = json.loads(line)\n",
    "        # lang_code = loaded_line.get('lang_code', None)\n",
    "        # if lang_code == 'en':\n",
    "        #     EER_words.append(loaded_line)\n",
    "        # elif lang_code == 'nl':\n",
    "        #     ENR_words.append(loaded_line)\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207186fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "file_path = ERAW_FILE\n",
    "def stream_json_lines(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Split if multiple JSON objects are jammed on one line\n",
    "            # (since each should start with {\"senses\":)\n",
    "            parts = re.split(r'(?=\\{\"senses\":)', line)\n",
    "            for part in parts:\n",
    "                part = part.strip()\n",
    "                if part:\n",
    "                    try:\n",
    "                        yield json.loads(part)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(\"Skipping malformed JSON:\", e, part[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893afe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(EER_words))\n",
    "print(len(error_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac0a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ENR_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682801ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(error_lines[50:70])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
